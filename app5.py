import os
import numpy as np
import pandas as pd
import streamlit as st
from dotenv import load_dotenv
from docx import Document
from pypdf import PdfReader
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sentence_transformers import SentenceTransformer
import google.generativeai as genai





# .envãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰
load_dotenv()

# APIã‚­ãƒ¼å–å¾—
api_key = st.secrets["GOOGLE_API_KEY"]
genai.configure(api_key=api_key)
if not api_key:
    st.error("APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚Google Cloudã®APIã‚­ãƒ¼ã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚")
    st.stop()

# ================================
# é–¢æ•°ã®è¨­å®š
# ================================
# ================================
# Geminiãƒ¢ãƒ‡ãƒ«å–å¾—
# ================================
@st.cache_resource
def get_gemini_model():
    return genai.GenerativeModel("models/gemini-flash-latest")

# ================================
# Word / PDF èª­ã¿è¾¼ã¿
# ================================
def load_documents_from_folder(folder_path):
    documents = []

    for file in os.listdir(folder_path):
        path = os.path.join(folder_path, file)

        if file.endswith(".docx"):
            content = load_word(path)
            documents.append({
                "content": content,
                "source": file,
                "type": "word",
                "location": "å…¨æ–‡"
            })

        elif file.endswith(".pdf"):
            content = load_pdf(path)
            documents.append({
                "content": content,
                "source": file,
                "type": "pdf",
                "location": "å…¨æ–‡"
            })

    return documents


def load_word(path):
    doc = Document(path)
    chunks = []

    for p in doc.paragraphs:
        if p.text.strip():
            chunks.append(p.text.strip())

    for table in doc.tables:
        for row in table.rows:
            row_text = " / ".join(
                cell.text.strip() for cell in row.cells if cell.text.strip()
            )
            if row_text:
                chunks.append(f"ã€è¡¨ã€‘{row_text}")

    return "\n".join(chunks)


def load_pdf(path):
    reader = PdfReader(path)
    chunks = []

    for i, page in enumerate(reader.pages):
        text = page.extract_text()
        if text:
            text = text.replace("å›³", "ã€å›³ã€‘").replace("è¡¨", "ã€è¡¨ã€‘")
            chunks.append(f"ã€ãƒšãƒ¼ã‚¸{i+1}ã€‘\n{text}")

    return "\n".join(chunks)





# ================================
# CSVèª­ã¿è¾¼ã¿
# ================================
@st.cache_data
def load_data(csv_file_path):
    df = pd.read_csv(csv_file_path)
    df = df.dropna(subset=["documents"])
    return df


# ================================
# TF-IDFãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰
# ================================
@st.cache_resource
def build_tfidf_model(document):
    tfidf_vectorizer = TfidfVectorizer(
        max_features=5000,
        ngram_range=(1, 2),
        stop_words="english"
    )
    tfidf_matrix = tfidf_vectorizer.fit_transform(document)
    return tfidf_vectorizer, tfidf_matrix



# ================================
# SentenceTransformerãƒ¢ãƒ‡ãƒ«å–å¾—
# ================================
@st.cache_resource
def get_embedding_model():
    model = SentenceTransformer("all-MiniLM-L6-v2")
    return model


# ================================
# åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«æ§‹ç¯‰
# ================================
@st.cache_resource
def build_embedding_model(document):
    model = get_embedding_model()
    embeddings = model.encode(document, show_progress_bar=True)
    return embeddings


# ================================
# ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢
# ================================
def hybrid_search(query, tfidf_matrix, tfidf_vectorizer, embeddings, top_n=5):
    # TF-IDFé¡ä¼¼åº¦
    query_tfidf = tfidf_vectorizer.transform([query])
    tfidf_scores = cosine_similarity(query_tfidf, tfidf_matrix)[0]

    # Embeddingé¡ä¼¼åº¦
    embed_model = get_embedding_model()
    query_embedding = embed_model.encode([query])
    embed_scores = cosine_similarity(query_embedding, embeddings)[0]

    # æ­£è¦åŒ–
    tfidf_scores = (tfidf_scores - tfidf_scores.min()) / (tfidf_scores.max() - tfidf_scores.min() + 1e-8)
    embed_scores = (embed_scores - embed_scores.min()) / (embed_scores.max() - embed_scores.min() + 1e-8)

    # ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã‚¹ã‚³ã‚¢
    hybrid_scores = (tfidf_scores + embed_scores) / 2

    top_indices = np.argsort(hybrid_scores)[::-1][:top_n]

    return top_indices, hybrid_scores[top_indices]



# ================================
# ãƒãƒ£ãƒƒãƒˆå±¥æ­´åˆæœŸåŒ–
# ================================
def init_chat_history():
    if "chat_history" not in st.session_state:
        st.session_state.chat_history = []


# ================================
# ãƒãƒ£ãƒƒãƒˆå±¥æ­´è¡¨ç¤º
# ================================
def display_chat_history():
    for role, message in st.session_state.chat_history:
        with st.chat_message(role):
            st.markdown(message)


# ================================
# Geminiã«ã‚ˆã‚‹å¿œç­”ç”Ÿæˆ
# ================================
def respond_with_gemini(query, results, documents, top_n=3):
    model = get_gemini_model()

    context_parts = []
    for idx in results[:top_n]:
        d = documents[idx]
        context_parts.append(
            f"ã€å‡ºå…¸ã€‘{d['source']}ï½œ{d['location']}\n{d['content']}"
        )

    context = "\n\n".join(context_parts)

    prompt = f"""

ã‚ãªãŸã¯ã€ç¤¾å†…è¦ç¨‹å°‚ç”¨AIã€‘ã§ã™ã€‚
ä»¥ä¸‹ã®ã€å‚ç…§æ–‡æ›¸ã€‘ã«è¨˜è¼‰ã•ã‚Œã¦ã„ã‚‹å†…å®¹ã®ã¿ã‚’æ ¹æ‹ ã¨ã—ã¦å›ç­”ã—ã¦ãã ã•ã„ã€‚

ã€å³å®ˆäº‹é …ã€‘
- å›³ãƒ»è¡¨ã®å†…å®¹ã‚‚æ–‡ç« æƒ…å ±ã¨ã—ã¦è§£é‡ˆã™ã‚‹ã“ã¨
- åˆ¤æ–­ã§ããªã„å ´åˆã¯ã€Œå‚ç…§æ–‡æ›¸ã«è¨˜è¼‰ãŒã‚ã‚Šã¾ã›ã‚“ã€ã¨æ˜ç¢ºã«è¨˜è¼‰ã™ã‚‹ã“ã¨

ã€å›ç­”ã®æ•´ç†ãƒ«ãƒ¼ãƒ«ã€‘
- å†…å®¹ã”ã¨ã«å¿…ãšè¦‹å‡ºã—ï¼ˆ##ï¼‰ã‚’ä»˜ã‘ã‚‹
- è¦‹å‡ºã—ã¯ã€Œçµè«–ãŒä¸€ç›®ã§åˆ†ã‹ã‚‹è¡¨ç¾ã€ã«ã™ã‚‹
- ç•°ãªã‚‹è©±é¡Œã‚’åŒä¸€è¦‹å‡ºã—ã«æ··åœ¨ã•ã›ãªã„

ã€æ–‡ç« è¡¨ç¾ãƒ«ãƒ¼ãƒ«ã€‘
- é•·æ–‡ã‚’1æ®µè½ã«ã¾ã¨ã‚ãªã„ï¼ˆ2ï½3è¡Œä»¥å†…ã§æ”¹è¡Œï¼‰
- é€£ç¶šã™ã‚‹æ–‡ç« ã«åŒä¸€è¡¨ç¾ã‚’å¤šç”¨ã—ãªã„ã“ã¨
- ã‚ã‹ã‚Šã‚„ã™ã„å£èªè¡¨ç¾ã«ã™ã‚‹ã“ã¨
- å˜èª¿ãªæ–‡ç« ã«ãªã‚‰ãªã„ã‚ˆã†ã«ã€æ–‡æœ«ã‚’ä½“è¨€æ­¢ã‚ã«ã—ãŸã‚Šã€Markdownå½¢å¼ã‚’ä½¿ç”¨ã™ã‚‹ãªã©ã—ã¦æ–‡ç« ã«ãƒªã‚ºãƒ æ„Ÿã‚’åŠ ãˆã‚‹ã“ã¨

ã€ç¦æ­¢äº‹é …ã€‘
- ã€Œã€œã¨è€ƒãˆã‚‰ã‚Œã‚‹ã€ã€Œã€œã¨æ€ã‚ã‚Œã‚‹ã€ãªã©ã®æ›–æ˜§è¡¨ç¾
- å‚ç…§æ–‡æ›¸ã®è¨˜è¼‰ã‚’è¶…ãˆãŸè¨€ã„æ›ãˆ


ã€å‚ç…§æ–‡æ›¸ã€‘
{context}

ã€è³ªå•ã€‘
{query}

ã€å›ç­”ã€‘
"""

    response = model.generate_content(prompt)
    return response.text



# ã‚¹ãƒ©ã‚¤ãƒ‰ä½œæˆ
def generate_slide_markdown(query, results, documents, top_n=5):
    model = get_gemini_model()

    context_parts = []
    for idx in results[:top_n]:
        d = documents[idx]
        context_parts.append(
            f"ã€å‡ºå…¸ã€‘{d['source']}ï½œ{d['location']}\n{d['content']}"
        )

    context = "\n\n".join(context_parts)

    prompt = f"""
ã‚ãªãŸã¯ã€ç¤¾å†…è³‡æ–™ã‚¹ãƒ©ã‚¤ãƒ‰ä½œæˆAIã€‘ã§ã™ã€‚
ä»¥ä¸‹ã®ã€å‚ç…§æ–‡æ›¸ã€‘ã®ã¿ã‚’æ ¹æ‹ ã«ã€ã‚¹ãƒ©ã‚¤ãƒ‰æ§‹æˆã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚

ã€ã‚¹ãƒ©ã‚¤ãƒ‰ä½œæˆãƒ«ãƒ¼ãƒ«ã€‘
- 1ã‚¹ãƒ©ã‚¤ãƒ‰ï¼1ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
- æ¨æ¸¬ãƒ»ä¸€èˆ¬è«–ã¯ç¦æ­¢
- ç¤¾å†…èª¬æ˜ãƒ»QCç™ºè¡¨å‘ã‘
- ç®‡æ¡æ›¸ãä¸­å¿ƒ
- æœ€å¤§10æšã¾ã§

ã€å‡ºåŠ›å½¢å¼ï¼ˆå³å®ˆï¼‰ã€‘
## ã‚¹ãƒ©ã‚¤ãƒ‰ã‚¿ã‚¤ãƒˆãƒ«
- è¦ç‚¹1
- è¦ç‚¹2
- è¦ç‚¹3

## ã‚¹ãƒ©ã‚¤ãƒ‰ã‚¿ã‚¤ãƒˆãƒ«
- è¦ç‚¹1
- è¦ç‚¹2

ã€å‚ç…§æ–‡æ›¸ã€‘
{context}

ã€ãƒ†ãƒ¼ãƒã€‘
{query}

ã€ã‚¹ãƒ©ã‚¤ãƒ‰æ§‹æˆã€‘
"""

    response = model.generate_content(prompt)
    return response.text



# wordã«è½ã¨ã—è¾¼ã‚€é–¢æ•°
def slide_markdown_to_word(slide_md, out_path):
    doc = Document()

    lines = slide_md.splitlines()

    for line in lines:
        line = line.strip()
        if not line:
            continue

        # ã‚¹ãƒ©ã‚¤ãƒ‰ã‚¿ã‚¤ãƒˆãƒ« â†’ è¦‹å‡ºã—
        if line.startswith("## "):
            title = line.replace("## ", "").strip()
            doc.add_heading(title, level=1)

        # ç®‡æ¡æ›¸ã
        elif line.startswith("- "):
            bullet = line.replace("- ", "").strip()
            doc.add_paragraph(bullet, style="List Bullet")

        else:
            # å¿µã®ãŸã‚é€šå¸¸æ–‡ã‚‚å—ã‘ã‚‹
            doc.add_paragraph(line)

    doc.save(out_path)




# ================================
# Streamlitã‚¢ãƒ—ãƒªã®ãƒ¡ã‚¤ãƒ³
# ================================
st.set_page_config(page_title="ç¤¾å†…è³‡æ–™AIæ¤œç´¢", layout="wide")

# ===== ç¤¾å†…å‘ã‘UIã‚¹ã‚¿ã‚¤ãƒ« =====
st.markdown("""
<style>
.chat-message-assistant {
    background-color: #f4f6f8;
}
.chat-message-user {
    background-color: #ffffff;
}
</style>
""", unsafe_allow_html=True)

st.title("ğŸ“˜ ç¤¾å†…è³‡æ–™æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ")

st.caption(
    "â€» æœ¬ãƒ„ãƒ¼ãƒ«ã¯ç¤¾å†…è¦ç¨‹ã®æ¤œç´¢è£œåŠ©ã‚’ç›®çš„ã¨ã—ã¦ã„ã¾ã™ã€‚"
    "æœ€çµ‚åˆ¤æ–­ã¯å¿…ãšåŸæ–‡ã‚’ã”ç¢ºèªãã ã•ã„ã€‚"
)


# ----------------
# ãƒ•ã‚©ãƒ«ãƒ€ã‹ã‚‰æ–‡æ›¸ãƒ­ãƒ¼ãƒ‰
# ----------------
FOLDER_PATH = r"C:\Users\mt100\Downloads\ãƒ†ã‚¹ãƒˆAIãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆ"


# ===== æ¤œç´¢ç”¨ corpusï¼ˆdocuments ã¨å®Œå…¨ä¸€è‡´ï¼‰=====
documents = load_documents_from_folder(FOLDER_PATH)

if not documents:
    st.error("æ–‡æ›¸ãŒã‚ã‚Šã¾ã›ã‚“")
    st.stop()

corpus = [d["content"] for d in documents]

tfidf_vectorizer, tfidf_matrix = build_tfidf_model(corpus)
embeddings = build_embedding_model(corpus)




# ----------------
# ãƒãƒ£ãƒƒãƒˆå±¥æ­´åˆæœŸåŒ–
# ----------------
init_chat_history()
display_chat_history()

# ----------------
# ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›
# ----------------
user_input = st.chat_input("è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")

if user_input:
    # ãƒ¦ãƒ¼ã‚¶ãƒ¼ç™ºè¨€ã‚’å±¥æ­´ã«è¿½åŠ 
    st.session_state.chat_history.append(("user", user_input))
    with st.chat_message("user"):
        st.markdown(user_input)
    
    # ----------------
    # å›ç­”ç”Ÿæˆä¸­è¡¨ç¤º
    # ----------------
    with st.spinner("ğŸ¤– å›ç­”ã‚’ç”Ÿæˆä¸­ã§ã™..."):
    
        # --------
        # æ¤œç´¢
        # --------
        top_indices, scores = hybrid_search(
        query=user_input,
        tfidf_matrix=tfidf_matrix,
        tfidf_vectorizer=tfidf_vectorizer,
        embeddings=embeddings,
        top_n=5
    )

        # æ¤œç´¢
        top_indices, scores = hybrid_search(
            query=user_input,
            tfidf_matrix=tfidf_matrix,
            tfidf_vectorizer=tfidf_vectorizer,
            embeddings=embeddings,
            top_n=5
        )

        valid_indices = list(top_indices)

        # â˜… ã“ã“ãŒé‡è¦
        st.session_state.valid_indices = valid_indices
        st.session_state.last_query = user_input


        # â˜… ã—ãã„å€¤ãªã—ï¼šä¸Šä½ã¯ã™ã¹ã¦ä½¿ã†
        valid_indices = list(top_indices)

        # --------
        # Geminiå¿œç­”ç”Ÿæˆ
        # --------
        answer_body = respond_with_gemini(
            query=user_input,
            results=valid_indices,
            documents=documents,
            top_n=3
        )


        final_answer = answer_body + "\n\n" 

        st.session_state.chat_history.append(("assistant", final_answer))
        with st.chat_message("assistant"):
            st.markdown(final_answer)

    # --------
    # AIå¿œç­”è¡¨ç¤º
    # --------
    with st.expander("ğŸ” å‚ç…§ã—ãŸæ–‡æ›¸"):
        rows = []
        for idx in valid_indices[:3]:
            d = documents[idx]
            rows.append({
                "ãƒ•ã‚¡ã‚¤ãƒ«": d["source"],
                "ç¨®åˆ¥": d["type"],
                "å‚ç…§ç®‡æ‰€": d["location"],
                "æŠœç²‹": d["content"][:200] + "â€¦"
            })

        st.dataframe(pd.DataFrame(rows), use_container_width=True)


# ----------------
# ã‚¹ãƒ©ã‚¤ãƒ‰ä½œæˆãƒœã‚¿ãƒ³
# ----------------
st.markdown("### ğŸ“Š è³‡æ–™åŒ–")

if st.button("ã“ã®è³ªå•ã‚’ã‚¹ãƒ©ã‚¤ãƒ‰ã«ã¾ã¨ã‚ã‚‹"):
    if "valid_indices" not in st.session_state:
        st.warning("å…ˆã«è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
        st.stop()

    with st.spinner("ğŸ“Š ã‚¹ãƒ©ã‚¤ãƒ‰æ§‹æˆã‚’ä½œæˆä¸­..."):
        slide_md = generate_slide_markdown(
            query=st.session_state.last_query,
            results=st.session_state.valid_indices,
            documents=documents,
            top_n=5
        )

    # â˜… session_state ã«ä¿å­˜
    st.session_state.slide_md = slide_md

# ----------------
# ã‚¹ãƒ©ã‚¤ãƒ‰æ§‹æˆæ¡ˆè¡¨ç¤º
# ----------------
if "slide_md" in st.session_state:
    st.markdown("## ğŸ§¾ ã‚¹ãƒ©ã‚¤ãƒ‰æ§‹æˆæ¡ˆ")
    st.markdown(st.session_state.slide_md)

# ----------------
# Word
# ----------------
if st.button("ğŸ“„ Wordã«å‡ºåŠ›"):
    if "slide_md" not in st.session_state:
        st.warning("å…ˆã«ã‚¹ãƒ©ã‚¤ãƒ‰æ§‹æˆæ¡ˆã‚’ä½œæˆã—ã¦ãã ã•ã„")
        st.stop()

    os.makedirs("output", exist_ok=True)
    word_path = "output/ã‚¹ãƒ©ã‚¤ãƒ‰æ§‹æˆæ¡ˆ.docx"

    slide_markdown_to_word(
        st.session_state.slide_md,
        word_path
    )

    st.download_button(
        label="ğŸ“¥ Wordã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
        data=open(word_path, "rb"),
        file_name="ã‚¹ãƒ©ã‚¤ãƒ‰æ§‹æˆæ¡ˆ.docx",
        mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document"
    )





